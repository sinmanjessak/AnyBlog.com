{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sinmanjessak/AnyBlog.com/blob/master/Hybrid_GANVAE_Full_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae86a650",
      "metadata": {
        "id": "ae86a650"
      },
      "source": [
        "\n",
        "# Hybrid VAE + WGAN‑GP (Final) — Cross‑Domain Time‑Series (ED + Finance)\n",
        "\n",
        "This is the **final, robust implementation**:\n",
        "- Trains **per‑domain baselines** (VAE, WGAN‑GP) on ED & Finance\n",
        "- Trains **Hybrid**: shared VAE encoder → conditional WGAN‑GP + domain‑adversarial alignment\n",
        "- Evaluates **quality** (ACF, PSD, DTW) and **utility** (forecasting RMSE/SMAPE)  \n",
        "- No external installs needed (custom DTW included)\n",
        "\n",
        "> Put these files next to the notebook: `ED_admission.csv`, `EUR_USD.csv`, `XAU_USD.csv`. The notebook will create `finance.csv` automatically from percentage returns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "dba7baf6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dba7baf6",
        "outputId": "07bad8e2-c99b-4cf7-b779-a129ae732ea7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os, random, json, math, gc, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "# ----------------\n",
        "# Configuration\n",
        "# ----------------\n",
        "ED_FILE = \"ED_admission.csv\"\n",
        "EUR_FILE = \"EUR_USD.csv\"\n",
        "XAU_FILE = \"XAU_USD.csv\"\n",
        "FIN_MERGED = \"finance.csv\"\n",
        "\n",
        "SEQ_LEN   = 60\n",
        "BATCH     = 64\n",
        "# Set solid defaults. You can reduce if compute is limited.\n",
        "EPOCHS_BASE   = 30   # per-domain VAE & WGAN-GP\n",
        "EPOCHS_HYBRID = 60   # joint hybrid training\n",
        "\n",
        "LR       = 2e-4\n",
        "BETAS    = (0.5, 0.9)\n",
        "N_CRITIC = 5\n",
        "LAMBDA_GP = 10.0\n",
        "\n",
        "NOISE_DIM = 16\n",
        "Z_SHARED  = 32\n",
        "Z_PRIV    = 8\n",
        "GEN_H = 96\n",
        "CRIT_H = 96\n",
        "VAE_H = 128\n",
        "\n",
        "FORECAST_HORIZON = 1\n",
        "LSTM_H = 96\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "196da6a5",
      "metadata": {
        "id": "196da6a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e3413f6-eb86-47b5-de3b-4c962cf9cb25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ED windows: (143221, 60)  Finance windows: (0, 60)\n"
          ]
        }
      ],
      "source": [
        "# -------------\n",
        "# Data helpers\n",
        "# -------------\n",
        "import os\n",
        "import numpy as np                # ✅ you were missing this\n",
        "import pandas as pd               # already needed\n",
        "\n",
        "# Configuration\n",
        "ED_FILE = \"ED_admission.csv\"\n",
        "EUR_FILE = \"EUR_USD.csv\"\n",
        "XAU_FILE = \"XAU_USD.csv\"\n",
        "FIN_MERGED = \"finance.csv\"\n",
        "SEQ_LEN = 60\n",
        "\n",
        "def read_single_series_csv(path):\n",
        "    df = pd.read_csv(path)\n",
        "    # try common columns\n",
        "    for c in ['value','close','Close','COUNT','count','y','price','Price']:\n",
        "        if c in df.columns:\n",
        "            ser = pd.to_numeric(df[c], errors='coerce').dropna()\n",
        "            return ser.values.astype(float)\n",
        "    # else last column (coerce)\n",
        "    ser = pd.to_numeric(df.iloc[:, -1], errors='coerce').dropna()\n",
        "    return ser.values.astype(float)\n",
        "\n",
        "def _normalize_date_cols(df: pd.DataFrame):\n",
        "    for dcol in ['date','Date','timestamp','Timestamp','datetime','Datetime']:\n",
        "        if dcol in df.columns:\n",
        "            df[dcol] = pd.to_datetime(df[dcol], errors='coerce')\n",
        "    return df\n",
        "\n",
        "def _to_numeric_series(s: pd.Series):\n",
        "    # remove thousands separators/commas then to numeric\n",
        "    return pd.to_numeric(s.astype(str).str.replace(',', '', regex=False), errors='coerce')\n",
        "\n",
        "def build_finance_csv(eur_path, xau_path, out_path=\"finance.csv\"):\n",
        "    \"\"\"\n",
        "    Builds a stationary, date-aligned finance.csv from EUR/USD and XAU/USD series.\n",
        "    Each row = aligned daily return across both assets.\n",
        "    \"\"\"\n",
        "    if not (os.path.exists(eur_path) and os.path.exists(xau_path)):\n",
        "        print(\"[WARN] EUR or XAU not found; skipping finance merge.\")\n",
        "        return None\n",
        "\n",
        "    eur = pd.read_csv(eur_path)\n",
        "    xau = pd.read_csv(xau_path)\n",
        "    eur = _normalize_date_cols(eur)\n",
        "    xau = _normalize_date_cols(xau)\n",
        "\n",
        "    def detect_price(df):\n",
        "        for c in ['close','Close','adj_close','Adj Close','value','price','Price','y','COUNT','count']:\n",
        "            if c in df.columns:\n",
        "                return c\n",
        "        # fallback: last numeric-ish column\n",
        "        for c in df.columns[::-1]:\n",
        "            if np.issubdtype(df[c].dtype, np.number):\n",
        "                return c\n",
        "        return df.columns[-1]  # will be coerced later\n",
        "\n",
        "    eur_col = detect_price(eur)\n",
        "    xau_col = detect_price(xau)\n",
        "\n",
        "    # Align by date if both have it\n",
        "    date_col = None\n",
        "    for c in ['date','Date','timestamp','Timestamp','datetime','Datetime']:\n",
        "        if c in eur.columns and c in xau.columns:\n",
        "            date_col = c\n",
        "            break\n",
        "\n",
        "    if date_col:\n",
        "        eur_use = eur[[date_col, eur_col]].rename(columns={eur_col: \"EUR_USD\"})\n",
        "        xau_use = xau[[date_col, xau_col]].rename(columns={xau_col: \"XAU_USD\"})\n",
        "        eur_use[\"EUR_USD\"] = _to_numeric_series(eur_use[\"EUR_USD\"])\n",
        "        xau_use[\"XAU_USD\"] = _to_numeric_series(xau_use[\"XAU_USD\"])\n",
        "        eur_use = eur_use.dropna()\n",
        "        xau_use = xau_use.dropna()\n",
        "        df = pd.merge(eur_use, xau_use, on=date_col, how='inner').sort_values(by=date_col).set_index(date_col)\n",
        "    else:\n",
        "        eur_vals = _to_numeric_series(eur[eur_col]).dropna().values\n",
        "        xau_vals = _to_numeric_series(xau[xau_col]).dropna().values\n",
        "        n = min(len(eur_vals), len(xau_vals))\n",
        "        df = pd.DataFrame({\n",
        "            \"EUR_USD\": eur_vals[-n:],\n",
        "            \"XAU_USD\": xau_vals[-n:]\n",
        "        })\n",
        "\n",
        "    # Debug (optional)\n",
        "    # print(\"DataFrame before pct_change:\\n\", df.head())\n",
        "    # print(\"dtypes:\", df.dtypes)\n",
        "\n",
        "    # Convert to percent returns (stationary)\n",
        "    df_ret = df.pct_change().replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "    # Save\n",
        "    df_ret.to_csv(out_path, index=bool(date_col))\n",
        "    print(f\"✅ Saved {out_path} — shape: {df_ret.shape}, columns: {list(df_ret.columns)}\")\n",
        "    # print(df_ret.head())\n",
        "    return out_path\n",
        "\n",
        "def zscore(x):\n",
        "    m = x.mean(); s = x.std() + 1e-8\n",
        "    return (x - m)/s, m, s\n",
        "\n",
        "def create_windows(arr, L):\n",
        "    if len(arr) < L: return np.empty((0, L))\n",
        "    out = [arr[i:i+L] for i in range(len(arr)-L+1)]\n",
        "    return np.stack(out)\n",
        "\n",
        "# Build (force rebuild if you want to overwrite an old file)\n",
        "if not os.path.exists(FIN_MERGED):\n",
        "    build_finance_csv(EUR_FILE, XAU_FILE, FIN_MERGED)\n",
        "\n",
        "# Load ED\n",
        "ed_raw = read_single_series_csv(ED_FILE)\n",
        "ed_norm, ed_mean, ed_std = zscore(ed_raw)\n",
        "ed_seq = create_windows(ed_norm, SEQ_LEN)\n",
        "\n",
        "# Load Finance (composite of EUR & XAU returns)\n",
        "if os.path.exists(FIN_MERGED):\n",
        "    fin_df = pd.read_csv(FIN_MERGED)\n",
        "    # If a Date column is present from the index, drop it for mean()\n",
        "    fin_df = fin_df.drop(columns=[c for c in fin_df.columns if c.lower() in (\"date\",\"timestamp\",\"datetime\")], errors='ignore')\n",
        "    fin_raw = fin_df.mean(axis=1).values.astype(float)\n",
        "    fin_norm, fin_mean, fin_std = zscore(fin_raw)\n",
        "    fin_seq = create_windows(fin_norm, SEQ_LEN)\n",
        "else:\n",
        "    fin_seq = np.empty((0, SEQ_LEN))\n",
        "\n",
        "print(\"ED windows:\", ed_seq.shape, \" Finance windows:\", fin_seq.shape)\n",
        "assert len(ed_seq) >= 200, \"ED dataset too small for stable training. Provide more points.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5bdcdf94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bdcdf94",
        "outputId": "4f80149d-902c-4fe5-dccb-6938325f2889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ED train/val: 128899 14322 | FIN train: 0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ----------------\n",
        "# Torch datasets\n",
        "# ----------------\n",
        "class SeqDS(Dataset):\n",
        "    def __init__(self, seqs, domain_id):\n",
        "        self.x = torch.tensor(seqs, dtype=torch.float32).unsqueeze(-1)\n",
        "        self.d = torch.full((len(self.x),), int(domain_id), dtype=torch.long)\n",
        "    def __len__(self): return len(self.x)\n",
        "    def __getitem__(self, i): return self.x[i], self.d[i]\n",
        "\n",
        "ed_ds = SeqDS(ed_seq, 0)\n",
        "ed_val_cut = max(1, int(0.1*len(ed_ds)))\n",
        "ed_train = torch.utils.data.Subset(ed_ds, range(len(ed_ds)-ed_val_cut))\n",
        "ed_val   = torch.utils.data.Subset(ed_ds, range(len(ed_ds)-ed_val_cut, len(ed_ds)))\n",
        "\n",
        "if len(fin_seq) > 0:\n",
        "    fin_train = SeqDS(fin_seq, 1)\n",
        "else:\n",
        "    fin_train = None\n",
        "\n",
        "print(\"ED train/val:\", len(ed_train), len(ed_val), \"| FIN train:\", 0 if fin_train is None else len(fin_train))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "889e5d52",
      "metadata": {
        "id": "889e5d52"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ----------------\n",
        "# VAE\n",
        "# ----------------\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, in_dim=1, h=128, z=32):\n",
        "        super().__init__()\n",
        "        self.enc = nn.LSTM(in_dim, h, batch_first=True)\n",
        "        self.mu   = nn.Linear(h, z)\n",
        "        self.logv = nn.Linear(h, z)\n",
        "        self.dec = nn.LSTM(z, h, batch_first=True)\n",
        "        self.out = nn.Linear(h, in_dim)\n",
        "        self.z = z\n",
        "    def encode(self, x):\n",
        "        _, (h, _) = self.enc(x); h = h[-1]\n",
        "        return self.mu(h), self.logv(h)\n",
        "    def reparam(self, mu, logv):\n",
        "        std = torch.exp(0.5*logv)\n",
        "        return mu + std*torch.randn_like(std)\n",
        "    def decode(self, z, seq_len):\n",
        "        zseq = z.unsqueeze(1).repeat(1, seq_len, 1)\n",
        "        y,_ = self.dec(zseq)\n",
        "        return self.out(y)\n",
        "    def forward(self, x):\n",
        "        mu, logv = self.encode(x)\n",
        "        z = self.reparam(mu, logv)\n",
        "        recon = self.decode(z, x.size(1))\n",
        "        return recon, mu, logv\n",
        "\n",
        "def vae_loss(recon, x, mu, logv, klw=1e-3):\n",
        "    rec = nn.MSELoss()(recon, x)\n",
        "    kld = -0.5*torch.mean(1 + logv - mu.pow(2) - logv.exp())\n",
        "    return rec + klw*kld, rec.item(), kld.item()\n",
        "\n",
        "def train_vae(ds, epochs):\n",
        "    vae = VAE(1, VAE_H, Z_SHARED).to(DEVICE)\n",
        "    opt = optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    loader = DataLoader(ds, batch_size=BATCH, shuffle=True, drop_last=True)\n",
        "    for ep in range(1, epochs+1):\n",
        "        s=0\n",
        "        for xb,_ in loader:\n",
        "            xb = xb.to(DEVICE)\n",
        "            recon, mu, logv = vae(xb)\n",
        "            loss, rec, kld = vae_loss(recon, xb, mu, logv, klw=5e-4 if ep>10 else 1e-4)\n",
        "            opt.zero_grad(); loss.backward()\n",
        "            nn.utils.clip_grad_norm_(vae.parameters(), 1.0)\n",
        "            opt.step(); s+=loss.item()\n",
        "        if ep%5==0 or ep==1:\n",
        "            print(f\"[VAE] {ep}/{epochs} loss={s/len(loader):.4f}\")\n",
        "    return vae\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "aad7ddc6",
      "metadata": {
        "id": "aad7ddc6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ----------------\n",
        "# WGAN‑GP (per domain)\n",
        "# ----------------\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, noise_dim, z_shared, z_priv, out_dim=1, h=96, conditional=True):\n",
        "        super().__init__()\n",
        "        self.conditional = conditional\n",
        "        in_dim = noise_dim + z_shared + (z_priv if conditional else 0) + (1 if conditional else 0)\n",
        "        self.lstm = nn.LSTM(in_dim, h, batch_first=True)\n",
        "        self.out = nn.Sequential(nn.Linear(h, out_dim))\n",
        "    def forward(self, noise, z_shared, domain, z_priv=None):\n",
        "        B,T,_ = noise.shape\n",
        "        feats = [noise, z_shared.unsqueeze(1).repeat(1,T,1)]\n",
        "        if self.conditional:\n",
        "            if z_priv is None: z_priv = torch.zeros(B, Z_PRIV, device=noise.device)\n",
        "            feats += [z_priv.unsqueeze(1).repeat(1,T,1),\n",
        "                      domain.view(B,1,1).float().repeat(1,T,1)]\n",
        "        x = torch.cat(feats, dim=2)\n",
        "        y,_ = self.lstm(x)\n",
        "        return self.out(y)\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, in_dim=1, h=96, conditional=True):\n",
        "        super().__init__()\n",
        "        self.conditional = conditional\n",
        "        self.lstm = nn.LSTM(in_dim + (1 if conditional else 0), h, batch_first=True)\n",
        "        self.fc = nn.Linear(h, 1)\n",
        "    def forward(self, seq, domain):\n",
        "        B,T,C = seq.shape\n",
        "        if self.conditional:\n",
        "            seq = torch.cat([seq, domain.view(B,1,1).float().repeat(1,T,1)], dim=2)\n",
        "        _,(h,_) = self.lstm(seq)\n",
        "        return self.fc(h[-1])\n",
        "\n",
        "def gradient_penalty(critic, real, fake, domain):\n",
        "    B = real.size(0)\n",
        "    alpha = torch.rand(B,1,1, device=real.device).expand_as(real)\n",
        "    inter = alpha*real + (1-alpha)*fake\n",
        "    inter.requires_grad_(True)\n",
        "    pred = critic(inter, domain)\n",
        "    grad = torch.autograd.grad(pred, inter, torch.ones_like(pred), create_graph=True, retain_graph=True)[0]\n",
        "    grad = grad.view(B, -1)\n",
        "    return ((grad.norm(2, dim=1) - 1.0)**2).mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0f40c494",
      "metadata": {
        "id": "0f40c494"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_wgan_gp(ds, epochs, conditional=False, domain_id=0):\n",
        "    gen = Generator(NOISE_DIM, Z_SHARED, Z_PRIV, 1, GEN_H, conditional=conditional).to(DEVICE)\n",
        "    crit= Critic(1, CRIT_H, conditional=conditional).to(DEVICE)\n",
        "    gopt= optim.Adam(gen.parameters(), lr=LR, betas=BETAS)\n",
        "    copt= optim.Adam(crit.parameters(), lr=LR, betas=BETAS)\n",
        "    loader = DataLoader(ds, batch_size=BATCH, shuffle=True, drop_last=True)\n",
        "    for ep in range(1, epochs+1):\n",
        "        gl=cl=0.0\n",
        "        for (x,d) in loader:\n",
        "            x=x.to(DEVICE); d=d.to(DEVICE)\n",
        "            B = x.size(0)\n",
        "            # Train critic\n",
        "            for _ in range(N_CRITIC):\n",
        "                z = torch.randn(B, SEQ_LEN, NOISE_DIM, device=DEVICE)\n",
        "                z_sh = torch.randn(B, Z_SHARED, device=DEVICE)\n",
        "                z_pr = torch.randn(B, Z_PRIV, device=DEVICE) if conditional else None\n",
        "                fake = gen(z, z_sh, d, z_pr)\n",
        "                r = crit(x, d if conditional else d*0)\n",
        "                f = crit(fake.detach(), d if conditional else d*0)\n",
        "                wass = f.mean() - r.mean()\n",
        "                gp = gradient_penalty(crit, x, fake.detach(), d if conditional else d*0)\n",
        "                loss_d = wass + LAMBDA_GP*gp\n",
        "                copt.zero_grad(); loss_d.backward()\n",
        "                nn.utils.clip_grad_norm_(crit.parameters(), 1.0)\n",
        "                copt.step(); cl += loss_d.item()\n",
        "            # Train generator\n",
        "            z = torch.randn(B, SEQ_LEN, NOISE_DIM, device=DEVICE)\n",
        "            z_sh = torch.randn(B, Z_SHARED, device=DEVICE)\n",
        "            z_pr = torch.randn(B, Z_PRIV, device=DEVICE) if conditional else None\n",
        "            fake = gen(z, z_sh, d, z_pr)\n",
        "            g_loss = -crit(fake, d if conditional else d*0).mean()\n",
        "            gopt.zero_grad(); g_loss.backward()\n",
        "            nn.utils.clip_grad_norm_(gen.parameters(), 1.0)\n",
        "            gopt.step(); gl += g_loss.item()\n",
        "        if ep%5==0 or ep==1:\n",
        "            print(f\"[WGAN] dom={domain_id} {ep}/{epochs}  D={cl/len(loader):.3f}  G={gl/len(loader):.3f}\")\n",
        "    return gen, crit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6422ff4",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "c6422ff4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ----------------\n",
        "# Train baselines\n",
        "# ----------------\n",
        "ed_vae = train_vae(ed_train, EPOCHS_BASE)\n",
        "if fin_train is not None:\n",
        "    fin_vae = train_vae(fin_train, max(10, EPOCHS_BASE//2))\n",
        "else:\n",
        "    fin_vae = None\n",
        "\n",
        "ed_gen_b, ed_crit_b = train_wgan_gp(ed_train, EPOCHS_BASE, conditional=False, domain_id=0)\n",
        "if fin_train is not None:\n",
        "    fin_gen_b, fin_crit_b = train_wgan_gp(fin_train, max(10, EPOCHS_BASE//2), conditional=False, domain_id=1)\n",
        "else:\n",
        "    fin_gen_b = fin_crit_b = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4899a7c",
      "metadata": {
        "id": "c4899a7c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ----------------\n",
        "# Hybrid (VAE encoder + conditional WGAN‑GP + domain GRL)\n",
        "# ----------------\n",
        "class GradReverse(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, lambd): ctx.lambd = lambd; return x.view_as(x)\n",
        "    @staticmethod\n",
        "    def backward(ctx, g): return -ctx.lambd * g, None\n",
        "\n",
        "class DomainHead(nn.Module):\n",
        "    def __init__(self, zdim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(nn.Linear(zdim, 64), nn.ReLU(), nn.Linear(64,2))\n",
        "    def forward(self, z, lambd=1.0): return self.net(GradReverse.apply(z, lambd))\n",
        "\n",
        "# unified dataset\n",
        "if fin_train is not None:\n",
        "    joint_x = np.concatenate([ed_seq, fin_seq], axis=0)\n",
        "    joint_d = np.concatenate([np.zeros(len(ed_seq)), np.ones(len(fin_seq))]).astype(int)\n",
        "else:\n",
        "    joint_x = ed_seq; joint_d = np.zeros(len(ed_seq)).astype(int)\n",
        "\n",
        "class JointDS(Dataset):\n",
        "    def __init__(self, x, d):\n",
        "        self.x = torch.tensor(x, dtype=torch.float32).unsqueeze(-1)\n",
        "        self.d = torch.tensor(d, dtype=torch.long)\n",
        "    def __len__(self): return len(self.x)\n",
        "    def __getitem__(self, i): return self.x[i], self.d[i]\n",
        "\n",
        "jloader = DataLoader(JointDS(joint_x, joint_d), batch_size=BATCH, shuffle=True, drop_last=True)\n",
        "\n",
        "hyb_gen  = Generator(NOISE_DIM, Z_SHARED, Z_PRIV, 1, GEN_H, conditional=True).to(DEVICE)\n",
        "hyb_crit = Critic(1, CRIT_H, conditional=True).to(DEVICE)\n",
        "hyb_dom  = DomainHead(Z_SHARED).to(DEVICE)\n",
        "\n",
        "g_opt = optim.Adam(hyb_gen.parameters(),  lr=LR, betas=BETAS)\n",
        "d_opt = optim.Adam(hyb_crit.parameters(), lr=LR, betas=BETAS)\n",
        "dom_opt = optim.Adam(hyb_dom.parameters(), lr=LR, betas=BETAS)\n",
        "\n",
        "def train_hybrid(epochs):\n",
        "    for ep in range(1, epochs+1):\n",
        "        dsum=gsum=ds=0.0\n",
        "        for xb,db in jloader:\n",
        "            xb=xb.to(DEVICE); db=db.to(DEVICE); B=xb.size(0)\n",
        "            # get shared z from the appropriate VAE encoder (randomly choose domain's VAE for variety if both exist)\n",
        "            with torch.no_grad():\n",
        "                if fin_vae is not None and torch.rand(1).item()<0.5:\n",
        "                    mu, logv = fin_vae.encode(xb)\n",
        "                    z_shared = fin_vae.reparam(mu, logv)\n",
        "                else:\n",
        "                    mu, logv = ed_vae.encode(xb)\n",
        "                    z_shared = ed_vae.reparam(mu, logv)\n",
        "            # train critic\n",
        "            for _ in range(N_CRITIC):\n",
        "                z = torch.randn(B, SEQ_LEN, NOISE_DIM, device=DEVICE)\n",
        "                z_pr = torch.randn(B, Z_PRIV, device=DEVICE)\n",
        "                fake = hyb_gen(z, z_shared, db, z_pr)\n",
        "                r = hyb_crit(xb, db); f = hyb_crit(fake.detach(), db)\n",
        "                wass = f.mean() - r.mean()\n",
        "                gp = gradient_penalty(hyb_crit, xb, fake.detach(), db)\n",
        "                d_loss = wass + LAMBDA_GP*gp\n",
        "                d_opt.zero_grad(); d_loss.backward()\n",
        "                nn.utils.clip_grad_norm_(hyb_crit.parameters(), 1.0)\n",
        "                d_opt.step(); dsum += d_loss.item()\n",
        "            # domain head (GRL) — encourage domain‑invariant z\n",
        "            dom_logits = hyb_dom(z_shared, lambd=1.0)\n",
        "            dom_loss = nn.CrossEntropyLoss()(dom_logits, db)\n",
        "            dom_opt.zero_grad(); dom_loss.backward(); dom_opt.step(); ds += dom_loss.item()\n",
        "            # generator\n",
        "            z = torch.randn(B, SEQ_LEN, NOISE_DIM, device=DEVICE)\n",
        "            z_pr = torch.randn(B, Z_PRIV, device=DEVICE)\n",
        "            fake = hyb_gen(z, z_shared, db, z_pr)\n",
        "            g_loss = -hyb_crit(fake, db).mean()\n",
        "            g_opt.zero_grad(); g_loss.backward()\n",
        "            nn.utils.clip_grad_norm_(hyb_gen.parameters(), 1.0)\n",
        "            g_opt.step(); gsum += g_loss.item()\n",
        "        if ep%5==0 or ep==1:\n",
        "            print(f\"[HYBRID] {ep}/{epochs}  D={dsum/len(jloader):.3f}  G={gsum/len(jloader):.3f}  Dom={ds/len(jloader):.3f}\")\n",
        "\n",
        "train_hybrid(EPOCHS_HYBRID)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e641581e",
      "metadata": {
        "id": "e641581e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ----------------\n",
        "# Evaluation: ACF / PSD / DTW + Forecast utility\n",
        "# ----------------\n",
        "def acf(seq, max_lag):\n",
        "    x = np.asarray(seq).ravel()\n",
        "    x = (x - x.mean())/(x.std()+1e-8)\n",
        "    out = [1.0]\n",
        "    for k in range(1, max_lag+1):\n",
        "        out.append(np.corrcoef(x[:-k], x[k:])[0,1])\n",
        "    return np.array(out)\n",
        "\n",
        "def psd(seq):\n",
        "    p = np.abs(np.fft.rfft(np.asarray(seq).ravel()))**2\n",
        "    return p/(p.sum()+1e-8)\n",
        "\n",
        "def dtw_dist(a, b):\n",
        "    a = np.asarray(a).ravel(); b = np.asarray(b).ravel()\n",
        "    n,m = len(a), len(b)\n",
        "    D = np.full((n+1,m+1), np.inf); D[0,0]=0.0\n",
        "    for i in range(1,n+1):\n",
        "        for j in range(1,m+1):\n",
        "            cost = abs(a[i-1]-b[j-1])\n",
        "            D[i,j] = cost + min(D[i-1,j], D[i,j-1], D[i-1,j-1])\n",
        "    return D[n,m]\n",
        "\n",
        "def sample_fake(gen, domain, n=128):\n",
        "    gen.eval()\n",
        "    with torch.no_grad():\n",
        "        z = torch.randn(n, SEQ_LEN, NOISE_DIM, device=DEVICE)\n",
        "        z_sh = torch.randn(n, Z_SHARED, device=DEVICE)\n",
        "        z_pr = torch.randn(n, Z_PRIV, device=DEVICE)\n",
        "        d = torch.full((n,), int(domain), dtype=torch.long, device=DEVICE)\n",
        "        fake = gen(z, z_sh, d, z_pr).cpu().numpy().squeeze(-1)\n",
        "    return fake\n",
        "\n",
        "real_ed = ed_seq[:256]\n",
        "fake_ed_b = sample_fake(ed_gen_b, 0, min(256, len(real_ed)))\n",
        "fake_ed_h = sample_fake(hyb_gen, 0, min(256, len(real_ed)))\n",
        "\n",
        "if len(fin_seq)>0 and fin_gen_b is not None:\n",
        "    real_fin = fin_seq[:256]\n",
        "    fake_fin_b = sample_fake(fin_gen_b, 1, min(256, len(real_fin)))\n",
        "    fake_fin_h = sample_fake(hyb_gen, 1, min(256, len(real_fin)))\n",
        "else:\n",
        "    real_fin = fake_fin_b = fake_fin_h = None\n",
        "\n",
        "def quality(real, fake, name):\n",
        "    max_lag = 20\n",
        "    r_acf = np.mean([acf(s, max_lag) for s in real], axis=0)\n",
        "    f_acf = np.mean([acf(s, max_lag) for s in fake], axis=0)\n",
        "    acf_delta = float(np.linalg.norm(r_acf - f_acf))\n",
        "\n",
        "    r_psd = np.mean([psd(s) for s in real], axis=0)\n",
        "    f_psd = np.mean([psd(s) for s in fake], axis=0)\n",
        "    psd_delta = float(np.linalg.norm(r_psd - f_psd))\n",
        "\n",
        "    # DTW: mean nearest neighbor distance (limit for speed)\n",
        "    ds = []\n",
        "    R = real[:64]; F = fake[:32]\n",
        "    for f in F:\n",
        "        ds.append(min(dtw_dist(f, r) for r in R))\n",
        "    dtw_mean = float(np.mean(ds))\n",
        "\n",
        "    print(f\"[{name}]  ACFΔ={acf_delta:.3f}  PSDΔ={psd_delta:.3f}  DTW̄={dtw_mean:.3f}\")\n",
        "    return {\"acf\": acf_delta, \"psd\": psd_delta, \"dtw\": dtw_mean}\n",
        "\n",
        "print(\"\\nQuality — ED\")\n",
        "m_ed_b = quality(real_ed, fake_ed_b, \"ED baseline\")\n",
        "m_ed_h = quality(real_ed, fake_ed_h, \"ED hybrid\")\n",
        "\n",
        "if real_fin is not None:\n",
        "    print(\"\\nQuality — Finance\")\n",
        "    m_fin_b = quality(real_fin, fake_fin_b, \"FIN baseline\")\n",
        "    m_fin_h = quality(real_fin, fake_fin_h, \"FIN hybrid\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3eb998a",
      "metadata": {
        "id": "d3eb998a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Forecasting utility (tiny LSTM, 10 epochs)\n",
        "class TinyLSTM(nn.Module):\n",
        "    def __init__(self, h=96):\n",
        "        super().__init__()\n",
        "        self.l = nn.LSTM(1, h, batch_first=True)\n",
        "        self.f = nn.Linear(h, 1)\n",
        "    def forward(self, x):\n",
        "        y,_ = self.l(x); return self.f(y[:,-1,:])\n",
        "\n",
        "def to_sup(seqs, horizon=1):\n",
        "    X=[];Y=[]\n",
        "    for s in seqs:\n",
        "        X.append(s[:-horizon]); Y.append(s[-horizon:][0])\n",
        "    X=np.array(X)[...,None]; Y=np.array(Y)[...,None]\n",
        "    return torch.tensor(X, dtype=torch.float32), torch.tensor(Y, dtype=torch.float32)\n",
        "\n",
        "def forecast_eval(train_set, test_set, tag):\n",
        "    Xtr,Ytr = to_sup(train_set); Xte,Yte = to_sup(test_set)\n",
        "    net = TinyLSTM(LSTM_H).to(DEVICE)\n",
        "    opt = optim.Adam(net.parameters(), lr=1e-3)\n",
        "    crit = nn.MSELoss()\n",
        "    bs=64\n",
        "    for ep in range(10):\n",
        "        idx = torch.randperm(len(Xtr))\n",
        "        for i in range(0, len(Xtr), bs):\n",
        "            xb = Xtr[idx[i:i+bs]].to(DEVICE); yb = Ytr[idx[i:i+bs]].to(DEVICE)\n",
        "            loss = crit(net(xb), yb); opt.zero_grad(); loss.backward(); opt.step()\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        pred = net(Xte.to(DEVICE)).cpu().numpy().ravel()\n",
        "        true = Yte.numpy().ravel()\n",
        "    rmse = float(np.sqrt(np.mean((pred-true)**2)))\n",
        "    smape = float(100*np.mean(2*np.abs(pred-true)/(np.abs(pred)+np.abs(true)+1e-8)))\n",
        "    print(f\"[Forecast {tag}] RMSE={rmse:.4f}  SMAPE={smape:.2f}%\")\n",
        "    return {\"rmse\": rmse, \"smape\": smape}\n",
        "\n",
        "# ED utility\n",
        "split_e = int(0.8*len(ed_seq))\n",
        "ed_train_sup, ed_test_sup = ed_seq[:split_e], ed_seq[split_e:]\n",
        "ed_aug = np.concatenate([ed_train_sup, fake_ed_h[:len(ed_train_sup)]], axis=0)\n",
        "\n",
        "print(\"\\nUtility — ED\")\n",
        "u_ed_real = forecast_eval(ed_train_sup, ed_test_sup, \"ED real-only\")\n",
        "u_ed_aug  = forecast_eval(ed_aug,       ed_test_sup, \"ED real+hybrid\")\n",
        "u_ed_syn  = forecast_eval(fake_ed_h[:len(ed_train_sup)], ed_test_sup, \"ED synth-only\")\n",
        "\n",
        "# Finance utility (if present & enough)\n",
        "if real_fin is not None and len(fin_seq) > 200:\n",
        "    split_f = int(0.8*len(fin_seq))\n",
        "    fin_train_sup, fin_test_sup = fin_seq[:split_f], fin_seq[split_f:]\n",
        "    fin_aug = np.concatenate([fin_train_sup, fake_fin_h[:len(fin_train_sup)]], axis=0)\n",
        "    print(\"\\nUtility — Finance\")\n",
        "    u_fin_real = forecast_eval(fin_train_sup, fin_test_sup, \"FIN real-only\")\n",
        "    u_fin_aug  = forecast_eval(fin_aug,       fin_test_sup, \"FIN real+hybrid\")\n",
        "    u_fin_syn  = forecast_eval(fake_fin_h[:len(fin_train_sup)], fin_test_sup, \"FIN synth-only\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8984f26",
      "metadata": {
        "id": "a8984f26"
      },
      "outputs": [],
      "source": [
        "\n",
        "summary = {\n",
        "    \"quality_ED\": {\"baseline\": m_ed_b, \"hybrid\": m_ed_h},\n",
        "    \"utility_ED\": {\"real\": u_ed_real, \"real+hybrid\": u_ed_aug, \"synth_only\": u_ed_syn},\n",
        "}\n",
        "try:\n",
        "    summary[\"quality_FIN\"] = {\"baseline\": m_fin_b, \"hybrid\": m_fin_h}\n",
        "except NameError:\n",
        "    pass\n",
        "print(\"\\n=== Summary (lower is better for all metrics) ===\")\n",
        "print(json.dumps(summary, indent=2))\n",
        "print(\"\\nExpectation: Hybrid should reduce ACF/PSD deltas and DTW vs baseline, and improve ED forecasting when augmenting with hybrid synthetic data.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}